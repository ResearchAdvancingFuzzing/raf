# This file will need to use a config file for the port that the service listens on

# make sure we do a helm install stable/postgresql BEFORE any of this since
# the setup requires that we have the postgres database already running 

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: pods-list
rules: 
- apiGroups: [""]  
  resources: ["nodes", "pods", "services", "secrets"] # what resources this Cluster Role applies to 
  verbs: ["list"] # what actions it can perform (right now just list things) 

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: pods-list
subjects:  # who this applies to: all serviceaccounts that are default (which is basically everything)
- kind: ServiceAccount
  name: default
  namespace: default
roleRef: # what role they are getting  
  kind: ClusterRole 
  name: pods-list
  apiGroup: rbac.authorization.k8s.io

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: dir-config
data:
  TARGET_DIR: /target
  CORPUS_DIR: /seed-corpus
  COVERAGE_DIR: /coverage
  SPITFIRE: /spitfire
  INPUTS_DIR: /inputs
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: data-pv 
spec: 
  #storageClassName: manual # bound by storage class name?
  capacity: 
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  hostPath: 
    path: "/mnt/data"

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: seed-corpus-pv-claim
spec:
  #storageClassName: manual
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: coverage-pv-claim
spec:
  #storageClassName: manual
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: spitfire-pv-claim
spec:
  #storageClassName: manual
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: target-pv-claim
spec:
  #storageClassName: manual
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: inputs-pv-claim
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 2Gi
---
kind: Job 
apiVersion: batch/v1
metadata: 
  name: init
spec:
  template:
    metadata:
      name: init
    spec: 
      containers:
        - name: init
          image: init:v1
          volumeMounts:
            - name: target-pv-storage
              mountPath: "/target"
            - name: seed-corpus-pv-storage
              mountPath: "/seed-corpus"
            - name: spitfire-pv-storage
              mountPath: "/spitfire"
            - name: coverage-pv-storage
              mountPath: "/coverage"
          envFrom: 
            - configMapRef: 
                name: dir-config 
      initContainers: 
        - name: get-target
          image: target:v1 #hpreslier/target:v1
          volumeMounts:
            - name: target-pv-storage
              mountPath: "/target"
        - name: get-seed-corpus
          image: seed-corpus:v1
          volumeMounts:
            - name: seed-corpus-pv-storage
              mountPath: "/seed-corpus"
        - name: get-spitfire
          image: spitfire:v1
          volumeMounts:
            - name: spitfire-pv-storage
              mountPath: "/spitfire"
        - name: get-coverage
          image: coverage:v1
          volumeMounts:
            - name: coverage-pv-storage
              mountPath: "/coverage" 
      volumes: # Pods access storage by using the claim as a volume. `
        - name: target-pv-storage
          persistentVolumeClaim:
            claimName: target-pv-claim 
        - name: seed-corpus-pv-storage
          persistentVolumeClaim:
            claimName: seed-corpus-pv-claim 
        - name: spitfire-pv-storage
          persistentVolumeClaim:
            claimName: spitfire-pv-claim
        - name: coverage-pv-storage
          persistentVolumeClaim:
            claimName: coverage-pv-claim 
      restartPolicy: OnFailure

# We need the grpc server as well; this is the only thing
# that will talk to the database
--- 
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: grpc-server
  labels:
    app: grpc-server
spec:
  selector:
    matchLabels:
      app: grpc-server
  template:
    metadata:
      labels:
        app: grpc-server
    spec:
      containers:
        - name: grpc-server
          image: knowledge-base:v1
          imagePullPolicy: Never 
          volumeMounts: 
            - name: spitfire-pv-storage
              mountPath: "/spitfire"
            - name: inputs-pv-storage
              mountPath: "/inputs"
          ports: 
            - name: server-port
              containerPort: 61111 # this shouldnt matter because we are going by name 
      volumes:
        - name: spitfire-pv-storage
          persistentVolumeClaim:
            claimName: spitfire-pv-claim
        - name: inputs-pv-storage
          persistentVolumeClaim:
            claimName: inputs-pv-claim

# This is the service for the grpc server; this is what each of the 
# grpc clients need to communicate through to the server 
---
apiVersion: v1
kind: Service
metadata:
  name: grpc-server-service
spec:
  clusterIP: "10.105.43.27" # for now
  selector:
    app: grpc-server
  ports: 
    - port: 61111
      targetPort: server-port  
---
