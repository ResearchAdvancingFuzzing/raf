# This file will need to use a config file for the port that the service listens on

# make sure we do a helm install stable/postgresql BEFORE any of this since
# the setup requires that we have the postgres database already running 

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: pods-list
rules: 
- apiGroups: [""]  
  resources: ["nodes", "pods", "services", "secrets"] # what resources this Cluster Role applies to 
  verbs: ["list"] # what actions it can perform (right now just list things) 

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: pods-list
subjects:  # who this applies to: all serviceaccounts that are default (which is basically everything)
- kind: ServiceAccount
  name: default
  namespace: default
roleRef: # what role they are getting  
  kind: ClusterRole 
  name: pods-list
  apiGroup: rbac.authorization.k8s.io

#---
# The fuzzing manager is the cron job that wakes up every x seconds,
# surveys the system and dispatches Kubernetes jobs if need be 
#apiVersion: batch/v1beta1 # version of the api to use (group/version)
#kind: CronJob
#metadata: # data that helps uniquely identify the object
#name: fuzzing-manager # must be unique within the namespace
  # labels: # used to organize&categorize; match selectors of controllers/services
  # app: fuzzing-manager
  #spec: # specification of the desired behavior of the Deployment
  #schedule: "*/1 * * * *" 
  #jobTemplate:
    #spec:
      #template:
        #spec:
          #containers:
            #- name: fuzzing_manager
            #image: hpreslier/fuzzing-manager:v1
            #restartPolicy: OnFailure 

# We need the grpc server as well; this is the only thing
# that will talk to the database
--- 
apiVersion: apps/v1
kind: Deployment
metadata: 
  name: grpc-server
  labels:
    app: grpc-server
spec:
  selector:
    matchLabels:
      app: grpc-server
  template:
    metadata:
      labels:
        app: grpc-server
    spec:
      containers:
        - name: grpc-server
          image: hpreslier/knowledge-base:v1
          imagePullPolicy: Always
          ports: 
            - name: server-port
              containerPort: 61111 # this shouldnt matter because we are going by name 

# This is the service for the grpc server; this is what each of the 
# grpc clients need to communicate through to the server 
---
apiVersion: v1
kind: Service
metadata:
  name: grpc-server-service
spec:
  clusterIP: "10.105.43.27" # for now
  selector:
    app: grpc-server
  ports: 
    - port: 61111
      targetPort: server-port  


# Basically, every job that the fuzzing-manager can potentially deploy 
# needs to be outlined as a job below with the docker image 


# This is the grpc client for panda/taint that the Cron Job can dispatch
#---
#apiVersion: batch/v1
#kind: Job
#metadata: 
  #name: taint-panda
#spec:
  #template:
    #metadata:
      #name: taint-panda
    #spec:
      #containers:
        #- name: taint-panda
        #image: hpreslier/fuzzer:v1 #knowledge-base-test:v1 
        #imagePullPolicy: Always 
      #restartPolicy: OnFailure

# This just needs to be a plain job to setup a database with the database tables using sqlalchemy 
# I feel like database should be in here for now
# We also want this to happen before the fuzzing-manager  
#---
#kind: Job 
#apiVersion: batch/v1
#metadata: 
  #name: init-db
  #labels:
    #app: init-db
#spec:
  #selector: 
    #matchLabels: 
      #app: init-db
  #template:
    #metadata:
      #name: init-db
      #labels:
        #app: init-db
    #spec: 
      #containers:
        #- name: init-db
        # image: hpreslier/setup-db:v1
        # imagePullPolicy: Always
      #restartPolicy: OnFailure

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: data-pv 
spec: 
  #storageClassName: manual # bound by storage class name?
  capacity: 
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  hostPath: 
    path: "/mnt/data"

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: seed-corpus-pv-claim
spec:
  #storageClassName: manual
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: coverage-pv-claim
spec:
  #storageClassName: manual
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: spitfire-pv-claim
spec:
  #storageClassName: manual
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: target-pv-claim
spec:
  #storageClassName: manual
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: gtfo-pv-claim
spec:
  #storageClassName: manual
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi

---
kind: Job 
apiVersion: batch/v1
metadata: 
  name: init
spec:
  template:
    metadata:
      name: init
    spec: 
      containers:
        - name: init
          image: init:v1
          volumeMounts:
            - name: target-pv-storage
              mountPath: "/target"
            - name: gtfo-pv-storage
              mountPath: "/gtfo-source"
            - name: seed-corpus-pv-storage
              mountPath: "/seed-corpus"
            - name: spitfire-pv-storage
              mountPath: "/spitfire"
            - name: coverage-pv-storage
              mountPath: "/coverage" 
      initContainers: 
        - name: get-target
          image: target:v1 #hpreslier/target:v1
          volumeMounts:
            - name: target-pv-storage
              mountPath: "/target"
        - name: get-gtfo-source
          image: gtfo-source:v1
          volumeMounts: 
            - name: gtfo-pv-storage
              mountPath: "/gtfo-source"
        - name: get-seed-corpus
          image: seed-corpus:v1
          volumeMounts:
            - name: seed-corpus-pv-storage
              mountPath: "/seed-corpus"
        - name: get-spitfire
          image: spitfire:v1
          volumeMounts:
            - name: spitfire-pv-storage
              mountPath: "/spitfire"
        - name: get-coverage
          image: coverage:v1
          volumeMounts:
            - name: coverage-pv-storage
              mountPath: "/coverage" 
      volumes: # Pods access storage by using the claim as a volume. `
        - name: target-pv-storage
          persistentVolumeClaim:
            claimName: target-pv-claim 
        - name: gtfo-pv-storage
          persistentVolumeClaim: 
            claimName: gtfo-pv-claim
        - name: seed-corpus-pv-storage
          persistentVolumeClaim:
            claimName: seed-corpus-pv-claim 
        - name: spitfire-pv-storage
          persistentVolumeClaim:
            claimName: spitfire-pv-claim
        - name: coverage-pv-storage
          persistentVolumeClaim:
            claimName: coverage-pv-claim 
      restartPolicy: OnFailure
---
kind: Job 
apiVersion: batch/v1
metadata: 
  name: fuzz
spec:
  template:
    metadata:
      name: fuzz
    spec: 
      containers:
        - name: fuzz
          image: fuzz:v1
          volumeMounts:
            - name: target-pv-storage
              mountPath: "/target"
            - name: gtfo-pv-storage
              mountPath: "/gtfo-source"
            - name: seed-corpus-pv-storage
              mountPath: "/seed-corpus"
            - name: spitfire-pv-storage
              mountPath: "/spitfire"
            - name: coverage-pv-storage
              mountPath: "/coverage"
      volumes: # Pods access storage by using the claim as a volume. `
        - name: target-pv-storage
          persistentVolumeClaim:
            claimName: target-pv-claim 
        - name: gtfo-pv-storage
          persistentVolumeClaim: 
            claimName: gtfo-pv-claim
        - name: seed-corpus-pv-storage
          persistentVolumeClaim:
            claimName: seed-corpus-pv-claim 
        - name: spitfire-pv-storage
          persistentVolumeClaim:
            claimName: spitfire-pv-claim
        - name: coverage-pv-storage
          persistentVolumeClaim:
            claimName: coverage-pv-claim
      restartPolicy: OnFailure
